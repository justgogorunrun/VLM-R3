{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VLM-R3 Interactive Reasoning Demo\n",
        "\n",
        "This notebook provides a polished, single-sample demonstration pipeline for the\n",
        "V*R-R3 visual reasoning agent.  It focuses on:\n",
        "\n",
        "- **Running** the agent on either an image or a video (the latter is converted\n",
        "  into a cinematic frame grid automatically).\n",
        "- **Capturing** the reasoning trace for every iteration, including tool calls\n",
        "  and bounding boxes.\n",
        "- **Visualising** each step with a sleek, high-tech styled layout that aligns\n",
        "  the textual explanation with the spatial grounding.\n",
        "- **Exporting** an artefact that can be consumed by the companion web demo to\n",
        "  reproduce the animated experience showcased on the project website.\n",
        "\n",
        "> \u26a0\ufe0f Running this notebook requires the VLM checkpoint, vLLM, and the video\n",
        "> dependencies (`PyAV`, FFmpeg) to be available in the execution environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "import shutil\n",
        "import sys\n",
        "import textwrap\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "\n",
        "import av\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from IPython.display import HTML, Markdown, display\n",
        "from matplotlib import patches\n",
        "from PIL import Image, ImageDraw\n",
        "from transformers import AutoProcessor\n",
        "from vllm import LLM\n",
        "\n",
        "plt.style.use(\"dark_background\")\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": (12, 6),\n",
        "    \"font.size\": 12,\n",
        "    \"axes.facecolor\": \"#05060A\",\n",
        "    \"savefig.facecolor\": \"#05060A\",\n",
        "})\n",
        "\n",
        "REPO_ROOT = Path.cwd().resolve()\n",
        "if not (REPO_ROOT / \"src\").exists():\n",
        "    REPO_ROOT = REPO_ROOT.parent\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "\n",
        "from qwen_vl_utils import smart_resize\n",
        "from src.model.r3_vllm import AgentVLMVLLM\n",
        "from src.utils.reasoning_viz import (\n",
        "    describe_run_for_export,\n",
        "    parse_reasoning_steps,\n",
        "    remap_bbox,\n",
        "    extract_answer_text,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== Configuration ======================================================\n",
        "MODEL_PATH = Path(\"/path/to/your/VLM-R3-7b-rl-v1\")\n",
        "MEDIA_PATH = Path(\"/path/to/your/media_file.mp4\")\n",
        "QUESTION = \"Describe what is happening in this scene.\"\n",
        "ANSWER_CHOICES: List[str] = []  # e.g. [\"(A) Option one\", \"(B) Option two\", ...]\n",
        "\n",
        "# vLLM / agent parameters\n",
        "DEVICE = \"cuda\"\n",
        "TENSOR_PARALLEL = 1\n",
        "GPU_MEMORY_UTILIZATION = 0.92\n",
        "MAX_ITERATIONS = 8\n",
        "\n",
        "# Visual sampling\n",
        "NUM_FRAMES = 256\n",
        "GRID_ROWS = 16\n",
        "GRID_COLS = 16\n",
        "\n",
        "# Vision encoder constraints (must match the checkpoint expectations)\n",
        "MIN_PIXELS = 32 * 28 * 28\n",
        "MAX_PIXELS = 8192 * 28 * 28\n",
        "CROP_MIN_PIXELS = 32 * 28 * 28\n",
        "CROP_MAX_PIXELS = 4096 * 28 * 28\n",
        "\n",
        "# Output bookkeeping\n",
        "OUTPUT_DIR = Path(\"demo_notebook_outputs\")\n",
        "RUN_NAME = \"demo_run\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VIDEO_EXTS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\", \".flv\", \".wmv\", \".mpg\", \".mpeg\"}\n",
        "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"}\n",
        "\n",
        "PALETTE = [\n",
        "    \"#19F7FF\", \"#FF6B9A\", \"#8CFF55\", \"#FDBF2D\",\n",
        "    \"#A855F7\", \"#2FD3C9\", \"#FF7847\", \"#63A4FF\",\n",
        "]\n",
        "\n",
        "\n",
        "def is_video(path: Path) -> bool:\n",
        "    return path.suffix.lower() in VIDEO_EXTS\n",
        "\n",
        "\n",
        "def decode_video_frames(video_path: Path) -> List[Image.Image]:\n",
        "    container = av.open(str(video_path))\n",
        "    frames: List[Image.Image] = []\n",
        "    try:\n",
        "        for frame in container.decode(video=0):\n",
        "            frames.append(frame.to_image().convert(\"RGB\"))\n",
        "    finally:\n",
        "        container.close()\n",
        "    if not frames:\n",
        "        raise RuntimeError(f\"No frames could be decoded from {video_path}\")\n",
        "    return frames\n",
        "\n",
        "\n",
        "def sample_frames_uniformly(frames: List[Image.Image], count: int) -> List[Image.Image]:\n",
        "    if count <= 0:\n",
        "        raise ValueError(\"count must be positive\")\n",
        "    if len(frames) <= count:\n",
        "        return list(frames)\n",
        "    indices = np.linspace(0, len(frames) - 1, count, dtype=int)\n",
        "    return [frames[idx] for idx in indices]\n",
        "\n",
        "\n",
        "def create_video_grid(video_path: Path, output_path: Path, num_frames: int) -> Path:\n",
        "    if GRID_ROWS * GRID_COLS != num_frames:\n",
        "        raise ValueError(\n",
        "            f\"Grid requires exactly {GRID_ROWS * GRID_COLS} frames, got {num_frames}.\"\n",
        "        )\n",
        "    frames = decode_video_frames(video_path)\n",
        "    sampled = sample_frames_uniformly(frames, num_frames)\n",
        "\n",
        "    width = sampled[0].width\n",
        "    height = sampled[0].height\n",
        "    grid = Image.new(\"RGB\", (width * GRID_COLS, height * GRID_ROWS))\n",
        "\n",
        "    for idx, frame in enumerate(sampled):\n",
        "        row = idx // GRID_COLS\n",
        "        col = idx % GRID_COLS\n",
        "        grid.paste(frame, (col * width, row * height))\n",
        "\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    grid.save(output_path)\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def format_reasoning_text(text: str, width: int = 70) -> str:\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return \"(No textual content emitted in this step.)\"\n",
        "    return \"\\n\".join(textwrap.wrap(text, width=width))\n",
        "\n",
        "\n",
        "def map_steps_for_display(steps, resized_size, base_size):\n",
        "    mapped = []\n",
        "    for step in steps:\n",
        "        bbox_display = None\n",
        "        if step.bbox is not None:\n",
        "            bbox_display = remap_bbox(step.bbox, resized_size, base_size)\n",
        "        mapped.append(\n",
        "            {\n",
        "                \"index\": step.index,\n",
        "                \"text\": step.text,\n",
        "                \"bbox_agent\": step.bbox,\n",
        "                \"bbox_display\": bbox_display,\n",
        "            }\n",
        "        )\n",
        "    return mapped\n",
        "\n",
        "\n",
        "def render_composite_figure(base_image_path: Path, mapped_steps):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    base = Image.open(base_image_path).convert(\"RGB\")\n",
        "    ax.imshow(base)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    for step in mapped_steps:\n",
        "        bbox = step[\"bbox_display\"]\n",
        "        if bbox is None:\n",
        "            continue\n",
        "        color = PALETTE[(step[\"index\"] - 1) % len(PALETTE)]\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        rect = patches.Rectangle(\n",
        "            (x1, y1),\n",
        "            x2 - x1,\n",
        "            y2 - y1,\n",
        "            linewidth=2.5,\n",
        "            edgecolor=color,\n",
        "            facecolor=\"none\",\n",
        "            linestyle=\"-\",\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(\n",
        "            x1,\n",
        "            max(0, y1 - 10),\n",
        "            f\"Step {step['index']}\",\n",
        "            color=color,\n",
        "            fontsize=11,\n",
        "            fontweight=\"bold\",\n",
        "            bbox=dict(facecolor=\"#05060A\", alpha=0.65, edgecolor=color, pad=2),\n",
        "        )\n",
        "\n",
        "    ax.set_title(\"Composite grounding overview\", fontsize=14, color=\"#ECF2FF\")\n",
        "    plt.tight_layout()\n",
        "    base.close()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def render_single_step(base_image_path: Path, mapped_steps, step_index: int):\n",
        "    step = mapped_steps[step_index - 1]\n",
        "    base = Image.open(base_image_path).convert(\"RGB\")\n",
        "    fig, axes = plt.subplots(1, 2, gridspec_kw={\"width_ratios\": [3.5, 2.2]})\n",
        "\n",
        "    axes[0].imshow(base)\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    color = PALETTE[(step_index - 1) % len(PALETTE)]\n",
        "    bbox = step[\"bbox_display\"]\n",
        "    if bbox is not None:\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        rect = patches.Rectangle(\n",
        "            (x1, y1),\n",
        "            x2 - x1,\n",
        "            y2 - y1,\n",
        "            linewidth=3,\n",
        "            edgecolor=color,\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        axes[0].add_patch(rect)\n",
        "        axes[0].text(\n",
        "            x1,\n",
        "            max(0, y1 - 12),\n",
        "            f\"Focus {step_index}\",\n",
        "            color=color,\n",
        "            fontsize=13,\n",
        "            fontweight=\"bold\",\n",
        "            bbox=dict(facecolor=\"#05060A\", alpha=0.7, edgecolor=color, pad=3),\n",
        "        )\n",
        "    else:\n",
        "        axes[0].text(\n",
        "            0.5,\n",
        "            0.5,\n",
        "            \"No crop used\",\n",
        "            transform=axes[0].transAxes,\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            color=\"#AAAAAA\",\n",
        "            fontsize=14,\n",
        "        )\n",
        "\n",
        "    axes[1].axis(\"off\")\n",
        "    axes[1].text(\n",
        "        0.05,\n",
        "        0.98,\n",
        "        format_reasoning_text(step[\"text\"]),\n",
        "        color=color,\n",
        "        fontsize=12,\n",
        "        fontfamily=\"monospace\",\n",
        "        va=\"top\",\n",
        "        ha=\"left\",\n",
        "        bbox=dict(facecolor=\"#0B111A\", edgecolor=color, boxstyle=\"round,pad=0.6\", alpha=0.9),\n",
        "    )\n",
        "\n",
        "    fig.suptitle(\n",
        "        f\"Reasoning step {step_index}\",\n",
        "        color=color,\n",
        "        fontsize=16,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    base.close()\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== Load processor, vLLM, and agent ===================================\n",
        "if 'processor' in globals():\n",
        "    print(\"\u2705 Reusing cached processor instance.\")\n",
        "else:\n",
        "    print(\"Loading processor from\", MODEL_PATH)\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        str(MODEL_PATH),\n",
        "        min_pixels=MIN_PIXELS,\n",
        "        max_pixels=MAX_PIXELS,\n",
        "    )\n",
        "\n",
        "if 'llm' in globals():\n",
        "    print(\"\u2705 Reusing cached vLLM instance.\")\n",
        "else:\n",
        "    print(\"Spawning vLLM...\")\n",
        "    llm = LLM(\n",
        "        model=str(MODEL_PATH),\n",
        "        device=DEVICE,\n",
        "        tensor_parallel_size=TENSOR_PARALLEL,\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "        dtype=torch.bfloat16,\n",
        "        limit_mm_per_prompt={\"image\": 16, \"video\": 0},\n",
        "        mm_processor_kwargs={\n",
        "            \"max_pixels\": MAX_PIXELS,\n",
        "            \"min_pixels\": MIN_PIXELS,\n",
        "        },\n",
        "        max_model_len=8192 * 4,\n",
        "    )\n",
        "\n",
        "if 'agent' in globals():\n",
        "    print(\"\u2705 Reusing cached AgentVLMVLLM instance.\")\n",
        "else:\n",
        "    agent = AgentVLMVLLM(\n",
        "        model=llm,\n",
        "        processor=processor,\n",
        "        temp_dir=str(OUTPUT_DIR / \"crops\"),\n",
        "        device=DEVICE,\n",
        "        min_pixels=MIN_PIXELS,\n",
        "        max_pixels=MAX_PIXELS,\n",
        "        temperature=0.0,\n",
        "        crop_min_pixels=CROP_MIN_PIXELS,\n",
        "        crop_max_pixels=CROP_MAX_PIXELS,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== Prepare media and run the agent ====================================\n",
        "media_path = MEDIA_PATH.expanduser().resolve()\n",
        "if not media_path.exists():\n",
        "    raise FileNotFoundError(f\"Media file not found: {media_path}\")\n",
        "\n",
        "if is_video(media_path):\n",
        "    grid_path = OUTPUT_DIR / f\"{media_path.stem}_grid_{NUM_FRAMES}.png\"\n",
        "    print(f\"Decoding video and building {GRID_ROWS}x{GRID_COLS} grid...\")\n",
        "    base_image_path = create_video_grid(media_path, grid_path, NUM_FRAMES)\n",
        "    media_kind = \"video_grid\"\n",
        "else:\n",
        "    base_image_path = media_path\n",
        "    media_kind = \"image\"\n",
        "\n",
        "base_image_path = base_image_path.resolve()\n",
        "base_image = Image.open(base_image_path).convert(\"RGB\")\n",
        "base_width, base_height = base_image.size\n",
        "resized_height, resized_width = smart_resize(\n",
        "    base_height,\n",
        "    base_width,\n",
        "    factor=28,\n",
        "    min_pixels=MIN_PIXELS,\n",
        "    max_pixels=MAX_PIXELS,\n",
        ")\n",
        "base_image.close()\n",
        "\n",
        "question_text = QUESTION.strip()\n",
        "if ANSWER_CHOICES:\n",
        "    question_text += \" \" + \" \".join(ANSWER_CHOICES)\n",
        "\n",
        "print(\"\n",
        "\ud83d\ude80 Launching multi-turn reasoning...\")\n",
        "response_list, crop_images, full_response, img_messages, text_messages = agent.process(\n",
        "    str(base_image_path),\n",
        "    question_text,\n",
        "    max_iterations=MAX_ITERATIONS,\n",
        ")\n",
        "\n",
        "raw_steps = parse_reasoning_steps(response_list)\n",
        "mapped_steps = map_steps_for_display(\n",
        "    raw_steps,\n",
        "    resized_size=(resized_width, resized_height),\n",
        "    base_size=(base_width, base_height),\n",
        ")\n",
        "final_answer = extract_answer_text(full_response)\n",
        "\n",
        "print(f\"\n",
        "Captured {len(mapped_steps)} reasoning segments.\")\n",
        "if final_answer:\n",
        "    print(\"Model answer:\", final_answer)\n",
        "else:\n",
        "    print(\"Model answer could not be extracted from <answer> tags.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== Summaries ==========================================================\n",
        "summary_lines = []\n",
        "for step in mapped_steps:\n",
        "    color = PALETTE[(step[\"index\"] - 1) % len(PALETTE)]\n",
        "    summary_lines.append(\n",
        "        f\"<span style='color:{color}; font-weight:600;'>Step {step['index']}:</span> \"\n",
        "        f\"<code>{step['text'].replace('\\n', ' ')}</code>\"\n",
        "    )\n",
        "\n",
        "html_template = '''\n",
        "<div style=\"background:#05060A;border:1px solid #1E2A3A;border-radius:8px;padding:16px;\">\n",
        "  <h3 style=\"color:#7BE6FF;margin-top:0;\">Reasoning Trace</h3>\n",
        "  <p style=\"color:#C8D5FF;\">{question}</p>\n",
        "  <ol style=\"color:#E0E7FF;\">\n",
        "    {items}\n",
        "  </ol>\n",
        "  <p style=\"color:#7BE6FF;font-weight:600;\">Final Answer: {answer}</p>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "html_summary = html_template.format(\n",
        "    question=question_text,\n",
        "    items=\"\\n\".join(f\"<li>{line}</li>\" for line in summary_lines),\n",
        "    answer=final_answer or \"(not available)\",\n",
        ")\n",
        "\n",
        "display(HTML(html_summary))\n",
        "fig = render_composite_figure(base_image_path, mapped_steps)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== Interactive step-by-step visualisation ============================\n",
        "if not mapped_steps:\n",
        "    display(Markdown(\"**No reasoning steps were captured.**\"))\n",
        "else:\n",
        "    slider = widgets.IntSlider(\n",
        "        min=1,\n",
        "        max=len(mapped_steps),\n",
        "        value=1,\n",
        "        step=1,\n",
        "        description=\"Step\",\n",
        "        continuous_update=False,\n",
        "        style={\"description_width\": \"initial\"},\n",
        "        layout=widgets.Layout(width=\"600px\"),\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def update_visual(change=None):\n",
        "        with output:\n",
        "            output.clear_output(wait=True)\n",
        "            fig = render_single_step(base_image_path, mapped_steps, slider.value)\n",
        "            display(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "    slider.observe(update_visual, names=\"value\")\n",
        "    display(slider)\n",
        "    display(output)\n",
        "    update_visual()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==== Persist payload for the web visualiser ===========================\n",
        "now = datetime.utcnow().isoformat() + \"Z\"\n",
        "export_payload = describe_run_for_export(\n",
        "    question=question_text,\n",
        "    media_path=media_path,\n",
        "    base_image_path=base_image_path,\n",
        "    base_size=(base_width, base_height),\n",
        "    resized_size=(resized_width, resized_height),\n",
        "    response_chunks=response_list,\n",
        "    full_response=full_response,\n",
        "    media_type=media_kind,\n",
        ")\n",
        "export_payload.update(\n",
        "    {\n",
        "        \"response_chunks\": response_list,\n",
        "        \"steps_agent_space\": [\n",
        "            {\n",
        "                \"index\": step[\"index\"],\n",
        "                \"bbox\": step[\"bbox_agent\"],\n",
        "                \"text\": step[\"text\"],\n",
        "            }\n",
        "            for step in mapped_steps\n",
        "        ],\n",
        "        \"resized_size\": {\"width\": resized_width, \"height\": resized_height},\n",
        "        \"question_raw\": QUESTION,\n",
        "        \"answer_choices\": ANSWER_CHOICES,\n",
        "        \"run\": {\n",
        "            \"timestamp\": now,\n",
        "            \"model_path\": str(MODEL_PATH),\n",
        "            \"device\": DEVICE,\n",
        "            \"max_iterations\": MAX_ITERATIONS,\n",
        "            \"num_frames\": NUM_FRAMES if media_kind == \"video_grid\" else None,\n",
        "        },\n",
        "    }\n",
        ")\n",
        "\n",
        "output_json = OUTPUT_DIR / f\"{RUN_NAME}_interactive_payload.json\"\n",
        "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(export_payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Payload saved to {output_json}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}